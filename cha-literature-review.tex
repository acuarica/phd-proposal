
\chapter{Literature Review}
\label{cha:literature-review}

Understanding how language features and \api{}s are being used is a broad topic.
There is plenty of research in computer science literature about empirical studies of programs; which involves several directions directly or indirectly related to our topic.
Along the last decades, researchers always has been interested in understanding what kind of programs developers write.
The motivation behind these studies is quite broad and --- together with the evolution of computer science itself --- has shifted to the needs of researchers.

For instance, already~\cite{knuthEmpiricalStudyFORTRAN1971} started to study \fortran{} programs.
By knowing what kind of programs arise in practice, a compiler optimizer can focus in those cases, and therefore can be more effective.
Alternatively, to measure the advantages between compilation and interpretation in \basic{}, \cite{hammondBASICEvaluationProcessing1977} has studied a representative dataset of programs.
Adding to Knuth's work, \cite{shenEmpiricalStudyFortran1990} made an empirical study for parallelizing compilers.
Similar works have been done for \cobol{}~\cite{salvadoriStaticProfileCOBOL1975,chevanceStaticProfileDynamic1978}, \pascal{}~\cite{cookContextualAnalysisPascal1982}, and \apl{}~\cite{saalPropertiesAPLPrograms1975,saalEmpiricalStudyAPL1977} programs. 

The organization of the rest of this chapter is as follows: 
How benchmarks and corpora relate to this kind of studies is presented in \S\ref{sec:lr:benchmarks}.
In \S\ref{sec:lr:largescale} we give an overview of other large-scale studies either in \java{} or in other languages.
Related to our cast study, in \S\ref{sec:lr:experiments} we show studies on how static type systems impact on programmers productivity.
Code Patterns discovery is presented in \S\ref{sec:lr:patterns}.
An overview of what tools are available to extract information from software repositories is given in \S\ref{sec:lr:mining},
while \S\ref{sec:lr:selection} shows how to select good candidates projects from a large-base software repository.

\section{Benchmarks and Corpora}
\label{sec:lr:benchmarks}

Benchmarks are crucial to properly evaluate and measure product development.
This is key for both research and industry.
One popular benchmark suite for \java{} is DaCapo~\cite{blackburnDaCapoBenchmarksJava2006}.
This suite has been already cited in more than thousand publications, showing how important is to have reliable benchmark suites.

Another suite has been developed by~\cite{temperoQualitasCorpusCurated2010}.
They provide a corpus of curated open source systems to facilitate empirical studies on source code.
On top of Qualitas Corpus,~\cite{dietrichXCorpusExecutableCorpus2017} provide an executable corpus of \java{} programs.
This allows any researcher to experiment with both static and dynamic analysis.

For any benchmark or corpus to be useful and reliable, it must faithfully represent real world code.
Along these lines, \cite{allamanisMiningSourceCode2013} go one step further and provide a large-scale (14,807) curated corpus of open source \java{} projects.

\section{Large-scale Codebase Empirical Studies}
\label{sec:lr:largescale}

In the same direction to our plan, \cite{callauHowWhyDevelopers2013} perform a study of the dynamic features of \smalltalk{}. 
Analogously, \cite{richardsAnalysisDynamicBehavior2010,richardsEvalThatMen2011} made a similar study, but in this case targeting \javascript{}'s dynamic behavior and in particular the \code{eval} function. 
Also for \javascript{}, \cite{madsenStringAnalysisDynamic2014} analyzed how fields are accessed via strings, while~\cite{jangEmpiricalStudyPrivacyviolating2010} analyzed privacy violations. 
Similar empirical studies were done for \php{}~\cite{hillsEmpiricalStudyPHP2013,dahseExperienceReportEmpirical2015,doyleEmpiricalStudyEvolution2011} and \swift{}~\cite{reboucasEmpiricalStudyUsage2016}.  

Going one step forward, \cite{rayLargescaleStudyProgramming2017} studied the correlation between programming languages and defects. 
One important note is that they choose relevant project by popularity, measured \emph{stars} in \github{}.
We argue that it is more important to analyse projects that are \emph{representative}, not \emph{popular}. 

For \java{},~\cite{dietrichContractsWildStudy2017a} made a study about how programmers use contracts in \mavencentral{}.
\cite{landmanChallengesStaticAnalysis2017} have analyzed the relevance of static analysis tools with respect to reflection.
They made an empirical study to check how often the reflection \api{} is used in real-world code.
They argue that controlled experiments on subjects need to be correlated with real-world use cases, \eg{}, \github{} or \mavencentral{}.
\cite{dietrichBrokenPromisesEmpirical2014} have studied how changes in \api{} library impact in \java{} programs.
Notice that they have used the Qualitas Corpus~\cite{temperoQualitasCorpusCurated2010} mentioned above for their study.

\subsection*{Exceptions}

\cite{keryExaminingProgrammerPractices2016,asaduzzamanHowDevelopersUse2016} focus on exceptions.
They made empirical studies on how programmers handle exceptions in \java{} code.
The work done by~\cite{nakshatriAnalysisExceptionHandling2016} categorized them in patterns.
Whether~\cite{coelhoUnveilingExceptionHandling2015} used a more dynamic approach by analysing stack traces and code issues in \github{}.

\cite{kechagiaUndocumentedUncheckedExceptions2014} analyzed how undocumented and
unchecked exceptions cause most of the exceptions in
Android applications.

\subsection*{Functional Programming Features}

Programming language design has been always a hot topic in computer science literature.
It has been extensively studied in the past decades.
For instance, there is a trend in incorporating functional programming features into mainstream object-oriented languages, \eg, lambdas in \java{} 8\footnote{\url{https://docs.oracle.com/javase/specs/jls/se8/html/jls-15.html\#jls-15.27}}, \cpp{}11\footnote{\url{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1968.pdf}} and \cs{} 3.0\footnote{\url{https://msdn.microsoft.com/en-us/library/bb308966.aspx\#csharp3.0overview\_topic7}}; or parametric polymorphism --- \ie{}, generics --- in \java{} 5\footnote{\url{https://docs.oracle.com/javase/1.5.0/docs/guide/language/generics.html}}\(^{,}\)\footnote{\url{http://www.oracle.com/technetwork/java/javase/generics-tutorial-159168.pdf}}.

\cite{mazinanianUnderstandingUseLambda2017} and \cite{uesbeckEmpiricalStudyImpact2016} studied how developers use lambdas in \java{} and \cpp{} respectively.
The inclusion of generics in \java{} is closely related to collections. 
\cite{parninJavaGenericsAdoption2011,parninAdoptionUseJava2013} studied how generics were adopted by \java{} developers.
They found that the use of generics do not significantly reduce the number of type casts.

\cite{costaEmpiricalStudyUsage2017} have mined \github{} corpus to study the use and performance of collections, and how these usages can be improved.
They have found out that in most cases there is an alternative usage that improves performance.

This kind of studies give an insight of the adoption of lambdas and generics; which can drive future direction for language designers and tool builders, while providing developers with best practices.


\section{Controlled Experiments on Subjects}
\label{sec:lr:experiments}

There is an extensive literature \perse{} in controlled experiments on subjects to understand several aspects in programming, and programming languages.
For instance, \cite{solowayEmpiricalStudiesProgramming1984} tried to understand the how expert programmers face problem solving.
\cite{buddTheoreticalEmpiricalStudies1980} made a empirical study on how effective is mutation testing.
\cite{precheltEmpiricalComparisonSeven2000} compared how a given --- fixed --- task was implemented in several programming languages.

\cite{latozaDevelopersAskReachability2010} realize that, in essence, programmers need to answer reachability questions to understand large codebases.

Several authors~\cite{stuchlikStaticVsDynamic2011,mayerEmpiricalStudyInfluence2012,harlinImpactUsingStaticType2017} measure whether using a static-type system improves programmers productivity.
They compare how a static and a dynamic type system impact on productivity.
The common setting for these studies is to have a set of programming problems.
Then, let a group of developers solve them in both a static and dynamic languages.

For these kind of studies to reflect reality, the problems to be solved need to be representative of the real-world code.
Having artificial problems may lead to invalid conclusions.
The work by~\cite{wuHowTypeErrors2017,wuLearningUserFriendly2017} goes towards this direction. 
They have examined programs written by students to understand real debugging conditions. 
Their focus is on ill-typed programs written in \haskell{}.
Unfortunately, these dataset does not correspond to real-world code. 

Therefore, it is important to study how casts are used in real-world code. 
This can led to informed decisions when designing these kind of experiments. 

\section{Code Patterns Discovery}
\label{sec:lr:patterns}

\cite{posnettTHEXMiningMetapatterns2010} have extended \asm{}~\cite{brunetonASMCodeManipulation2002,kuleshovUsingASMFramework2007} to implement symbolic execution and recognize call sites.
However, this is only a meta-pattern detector, and not a pattern discovery.
\cite{huDynamicAnalysisDesign2008} used both dynamic and static analysis to discover design patterns, while \cite{arcelliDesignPatternDetection2008} used only dynamic.

Trying to unify analysis and transformation tools~\cite{vinjuHowMakeBridge2006}, \cite{klintRASCALDomainSpecific2009} built \rascal, a DSL that aims to bring them together.

As mentioned above, \cite{dietrichContractsWildStudy2017a} made a study about how programmers use contracts in \mavencentral{}.
For their analysis\footnote{\url{https://bitbucket.org/jensdietrich/contractstudy}}, they have use JavaParser\footnote{\url{http://javaparser.org/}}.
The main issue with JavaParser is the lack to do symbol resolution integrated with the project dependencies.

\cite{urmaProgrammingLanguageEvolution2012} evaluates seven source code query languages\footnote{\url{https://wiki.openjdk.java.net/display/Compiler/Java+Corpus+Tools}}:
\emph{Java Tools Language} \cite{cohenJTLJavaTools},
\emph{Browse-By-Query} \footnote{\url{http://browsebyquery.sourceforge.net/}},
\emph{SOUL} \cite{derooverSOULToolSuite2011},
\emph{JQuery} \cite{volderJqueryGenericCode2006a},
\emph{.QL} \cite{moorKeynoteAddressQL2007},
\emph{Jackpot} \footnote{\url{http://wiki.netbeans.org/Jackpot}}, and
\emph{PMD} \footnote{\url{https://pmd.github.io/}}.
They have implemented --- whenever possible --- four use cases using the tools mentioned above.
They concluded that only \emph{SOUL} and \emph{.QL} have the minimal features to implement all their use cases.

\section{Tools for Mining Software Repositories}
\label{sec:lr:mining}

When talking about mining software repositories, we refer to extracting any kind of information from large-scale codebase repositories. 
Usually doing so requires several engineering but challenging tasks.
The most common being downloading, storing, parsing, analyzing and properly extracting different kinds of artifacts.
In this scenario, there are several tools that allows a researcher or developer to query information about software repositories.

\cite{dyerBoaLanguageInfrastructure2013,dyerDeclarativeVisitorsEase2013} built \boa{}, both a domain-specific language and an online platform\footnote{\url{http://boa.cs.iastate.edu/}}. 
It is used to query software repositories on two popular hosting services, \github \footnote{\url{https://github.com/}} and \sourceforge \footnote{\url{https://sourceforge.net/}}.
The same authors of \boa{} made a study on how new features in \java{} were adopted by developers~\cite{dyerMiningBillionsAST2014}.
This study is based \sourceforge{} data.
The current problem with \sourceforge{} is that is outdated.

To this end, \cite{gousiosGHTorentDatasetTool2013} provides an offline mirror of \github{} that allows researchers to query any kind of that data.
Later on, \cite{gousiosLeanGHTorrentGitHub2014} published the dataset construction process of \github{}.

Similar to \boa{}, \lgtm{}\footnote{\url{https://lgtm.com/}} is a platform to query software projects properties.
It works by querying repositories from \github{}.
But it does not work at a large-scale, \ie{}, \lgtm{} allows the user to query just a few projects.
Unlike \boa{}, \lgtm{} is based on \ql{} --- before named \emph{.QL} as mentioned in \S\ref{sec:lr:patterns} ---, an object-oriented domain-specific language to query recursive data structures~\cite{avgustinovQLObjectorientedQueries2016}.

On top of \boa{},~\cite{tiwariCandoiaPlatformBuilding2017} built \candoia \footnote{\url{http://candoia.github.io/}}. 
Although it is not a mining software repository \perse{}, it eases the creation of mining applications. 

Another tool to analyze large software repositories is presented in~\cite{brandauerSpencerInteractiveHeap2017}.
In this case, the analysis is dynamic, based on program traces. 
At the time of this writing, the service\footnote{\url{http://www.spencer-t.racing/datasets}} was unavailable for testing. 

\cite{bajracharyaSourcererInternetscaleSoftware2009} provide a tool to query large code bases by extracting the source code into a relational model.

\sourcegraph{}\footnote{\url{https://sourcegraph.com}} is a tool that allows regular expression and diff searches.
It integrates with source repositories.

\section{Selecting Good Representatives}
\label{sec:lr:selection}

Another dimension to consider when analyzing large codebases, is how relevant the repositories are.
\cite{lopesDeJaVuMapCode2017} made a study to measure code duplication in \github{}.
They found out that much of the code there is actually duplicated.
This raises a flag when consider which projects analyze when doing mining software repositories.

\cite{nagappanDiversitySoftwareEngineering2013} have developed the Software Projects Sampling (SPS) tool.
SPS tries to find a maximal set of projects based on representativeness and diversity.
Diversity dimensions considered include total lines of code, project age, activity, and of the last 12 months, number of contributors, total code churn, and number of commits.